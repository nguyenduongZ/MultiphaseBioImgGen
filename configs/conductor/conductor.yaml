mode: training

unet_number: 1
cond_scale: 3  # 1 to 10 for training or testing
sample_quantity: 1
seed: 42
lr: 1e-4

config_index:
  - [0, 1, 2, 3]
  
### Trainer
trainer:
  ## DataLoader:
  batch_size: 16
  num_workers: 4
  pin_memory: True
  shuffle: True

  ## Training
  idx: 0
  iterations: 200000
  epochs: 100
  
  early_stopping:
    patience: 10000
    min_delta: 1e-3

  split_valid_from_train: False
  dl_tuple_output_keywords_names: [images, text_embeds]

  PATH_MODEL_CHECKPOINT: ./results/training/vindr_multiphase_imagen/unet1/cond_scale_2/2025-04-27_13-29-49/checkpoints/checkpoint-final.pt
  PATH_MODEL_LOAD: ./results/training/vindr_multiphase_imagen/unet1/cond_scale_2/2025-04-27_13-29-49/checkpoints/checkpoint-final.pt
  PATH_MODEL_SAVE: ./results/training/VinDrMultiphase_Imagen_unet2/cond_scale_2/model/fold1_checkpoint.pt

### Validation
validation:
  display_samples: True
  max_checkpoints: 3
    
  interval:
    valid_loss: 100
    validate_model: 40000

### Testing
testing:
  PATH_MODEL_TESTING: ./results/training/vindr_multiphase_imagen/unet1/cond_scale_2/2025-04-27_13-29-49/checkpoints/checkpoint-final.pt

  save_samples: True
  save_image_tensors: True
  loss_weighting: p2
  # text:
  # phase:

  # Define Batch Boundaries to parallize sampling
  lower_batch: 0
  upper_batch: 160
  batch_interval: 20

  # Frechet Inception Distance
  FrechetInceptionDistance:
    usage: True
    params:
      feature: 2048
      reset_real_features: True
      normalize: True

  # Kernel Inception Distance
  KernelInceptionDistance:
    usage: True
    params:
      feature: 2048
      subsets: 50
      subset_size: 512
      reset_real_features: True
      normalize: True

  # Clean Fréchet Inception Distance
  CleanFID:
    usage: True
    params:
      mode: clean
      model_name: inception_v3
      num_workers: 4
      batch_size: 32 

  # Fréchet CLIP Distance
  FrechetCLIPDistance:
    usage: True
    params:
      mode: clean
      model_name: clip_vit_b_32
      num_workers: 4
      batch_size: 32 

  #
  LearnedPerceptualImagePatchSimilarity:
    usage: False
    params:
      net_type: vgg

  # CMMD
  CMMD:
    usage: True
    params:
      model_name: openai/clip-vit-large-patch14-336
      subsets: 50
      subset_size: 512
      normalize: True